[{"question_id": 1, "language": "python", "file_name": "linear_regression.py", "question": "import torch\nimport torch.nn as nn\n\nclass LinearRegression(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LinearRegression, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    def forward(self, x):\n        \"\"\"\n        完成二元回归的前向传播过程\n\n        输入:\n        - x: 一个形状为(batch_size, input_size)的PyTorch张量。\n\n        输出:\n        - out: 一个形状为(batch_size, output_size)的PyTorch张量，表示线性回归模型的输出。\n        \"\"\"\n         # TODO: 完成这个函数", "reference": "import torch\nimport torch.nn as nn\n\nclass LinearRegression(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LinearRegression, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    def forward(self, x):\n        \"\"\"\n        输入:\n        - x: 一个形状为(batch_size, input_size)的PyTorch张量。\n\n        输出:\n        - out: 一个形状为(batch_size, output_size)的PyTorch张量，表示线性回归模型的输出。\n        \"\"\"\n        out = self.linear(x)\n        return out", "main": "import torch\nimport torch.nn as nn\nimport numpy as np\nfrom linear_regression import LinearRegression\n\n\n\n# 定义模型\ninput_dim = 1  # 输入的维度\noutput_dim = 1  # 输出的维度\n\nmodel = LinearRegression(input_dim, output_dim)\n\n# 定义损失函数和优化器\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# 训练模型\nepochs = 100\n# 数据集\nnp.random.seed(0)\nx = np.random.rand(10, 1)\ny = 2 * x + 3 + np.random.randn(10, 1)\n\n# 将numpy数组转换为PyTorch Tensor\nx = torch.from_numpy(x).float()\ny = torch.from_numpy(y).float()\n\nfor epoch in range(epochs):\n    # 前向传播\n    outputs = model(x)\n    loss = criterion(outputs, y)\n    \n    # 反向传播和优化\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1) % 10 == 0:\n        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\nprint(\"Final Loss:\",loss.item())\n# 在训练循环结束后\nassert loss.item() < 2, \"Loss is not less than 2\""}, {"question_id": 2, "language": "python", "file_name": "image_classifier.py", "question": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\nclass ImageClassifier(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(ImageClassifier, self).__init__()\n        self.conv = nn.Conv2d(input_size, 32, kernel_size=3, stride=1, padding=1)\n        self.fc = nn.Linear(32*32*32, num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        输入:\n        - x: 一个形状为(batch_size, input_size, height, width)的PyTorch张量。\n\n        输出:\n        - out: 一个形状为(batch_size, num_classes)的PyTorch张量，表示图像分类模型的输出。\n        \"\"\"\n        \"\"\"\n        TODO:\n        1. 使用`self.conv`对输入`x`进行卷积操作。\n        2. 对卷积的结果进行ReLU激活函数操作。\n        3. 对激活函数的结果进行平均池化操作。\n        4. 将池化的结果展平(flatten)。\n        5. 使用`self.fc`对展平的结果进行线性变换。\n        6. 将线性变换的结果返回。\n\n         注意:\n        - 你应该使用`self.conv`和`self.fc`来进行计算。\n        - 不要使用任何形式的循环，包括for循环和while循环。\n        - 不要使用`.item()`，它会使得你的代码无法进行梯度反向传播。\n        \"\"\"", "reference": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\nclass ImageClassifier(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(ImageClassifier, self).__init__()\n        self.conv = nn.Conv2d(input_size, 32, kernel_size=3, stride=1, padding=1)\n        self.fc = nn.Linear(32*16*16, num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        输入:\n        - x: 一个形状为(batch_size, input_size, height, width)的PyTorch张量。\n\n        输出:\n        - out: 一个形状为(batch_size, num_classes)的PyTorch张量，表示图像分类模型的输出。\n        \"\"\"\n         # 卷积操作\n        x = self.conv(x)\n        # ReLU激活函数\n        x = torch.relu(x)\n        # 平均池化操作\n        x = nn.functional.avg_pool2d(x, 2)\n        # 展平操作\n        x = x.view(x.size(0), -1)\n        # 线性变换\n        out = self.fc(x)\n        return out", "main": "import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom image_classifier import ImageClassifier\n\n# 定义超参数\ninput_size = 3  # 输入的维度（对于彩色图像，通常是3）\nnum_classes = 10  # 输出的维度（对于CIFAR-10数据集，有10个类别）\nnum_epochs = 3  # 训练的轮数\nbatch_size = 100  # 每个批次的样本数量\nlearning_rate = 0.001  # 学习率\n\n# 加载CIFAR-10数据集\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data/AItest/data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# 定义模型\nmodel = ImageClassifier(input_size, num_classes)\n\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# 训练模型\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # 前向传播\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # 反向传播和优化\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\nprint(\"最后的Loss:\",loss.item())\nassert loss.item() < 2, \"Loss is not less than 2\""}]